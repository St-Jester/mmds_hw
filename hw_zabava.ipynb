{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover,Word2Vec,BucketedRandomProjectionLSH\n",
    "from pyspark.ml import Pipeline, Estimator, Model\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,Evaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable \n",
    "from pyspark.ml.param.shared import *\n",
    "from pyspark import keyword_only \n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-19 00:03:08--  http://data.insideairbnb.com/spain/catalonia/barcelona/2020-10-12/visualisations/listings.csv\n",
      "Resolving data.insideairbnb.com (data.insideairbnb.com)... 52.216.88.178\n",
      "Connecting to data.insideairbnb.com (data.insideairbnb.com)|52.216.88.178|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3070172 (2.9M) [application/csv]\n",
      "Saving to: ‘listings_barc.csv’\n",
      "\n",
      "listings_barc.csv   100%[===================>]   2.93M  1.36MB/s    in 2.2s    \n",
      "\n",
      "2020-11-19 00:03:11 (1.36 MB/s) - ‘listings_barc.csv’ saved [3070172/3070172]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"http://data.insideairbnb.com/spain/catalonia/barcelona/2020-10-12/visualisations/listings.csv\" -O listings_barc.csv\n",
    "# !wget http://data.insideairbnb.com/spain/catalonia/barcelona/2020-10-12/visualisations/reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# use pandas to read and clean data, because spark has some issues reading the csv file\n",
    "df = pd.read_csv(\"listings_barc.csv\")\n",
    "# drop all na so no errors like \n",
    "df.dropna(inplace = True)\n",
    "\n",
    "# make indexes start form 1\n",
    "df['id'] = np.arange(1, len(df) + 1)\n",
    "\n",
    "# df.to_csv(\"listings_barc.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_clear = spark.createDataFrame(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to get KNN_model (returned in CustomLSH::_fit)\n",
    "class HasKnnModel(Params):\n",
    "    knn_model = Param(Params._dummy(), \"knn_model\", \"knn_model\")\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasKnnModel, self).__init__()\n",
    "\n",
    "    def setKnnModel(self, value):\n",
    "        return self._set(knn_model=value)\n",
    "\n",
    "    def getKnnModel(self):\n",
    "        return self.getOrDefault(self.knn_model)\n",
    "\n",
    "class HasNumHashTables(Params):\n",
    "    numHashTables = Param(Params._dummy(), \"numHashTables\", \"numHashTables\", \n",
    "        typeConverter=TypeConverters.toInt)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasNumHashTables, self).__init__()\n",
    "\n",
    "    def setNumHashTables(self, value):\n",
    "        return self._set(numHashTables=value)\n",
    "\n",
    "    def getNumHashTables(self):\n",
    "        return self.getOrDefault(self.numHashTables)\n",
    "    \n",
    "class HasLshModel(Params):\n",
    "    lshModel = Param(Params._dummy(), \"lshModel\", \"lshModel\")\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasLshModel, self).__init__()\n",
    "\n",
    "    def setLshModel(self, value):\n",
    "        return self._set(lshModel=value)\n",
    "\n",
    "    def getLshModel(self):\n",
    "        return self.getOrDefault(self.lshModel)\n",
    "    \n",
    "class HasTrainDataset(Params):\n",
    "    trainDataset = Param(Params._dummy(), \"trainDataset\", \"trainDataset\")\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasTrainDataset, self).__init__()\n",
    "\n",
    "    def setTrainDataset(self, value):\n",
    "        return self._set(trainDataset=value)\n",
    "\n",
    "    def getTrainDataset(self):\n",
    "        return self.getOrDefault(self.trainDataset)\n",
    "    \n",
    "    \n",
    "class HasBucketLength(Params):\n",
    "    bucketLength = Param(Params._dummy(), \"bucketLength\", \"bucketLength\", \n",
    "        typeConverter=TypeConverters.toInt)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasBucketLength, self).__init__()\n",
    "\n",
    "    def setBucketLength(self, value):\n",
    "        return self._set(bucketLength=value)\n",
    "\n",
    "    def getBucketLength(self):\n",
    "        return self.getOrDefault(self.bucketLength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomLSH_Model(Model, HasInputCol, HasPredictionCol,\n",
    "        HasNumHashTables, HasLshModel,HasTrainDataset,\n",
    "        DefaultParamsReadable, DefaultParamsWritable, HasBucketLength, HasKnnModel):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self,inputCol=None, predictionCol=None,\n",
    "                numHashTables=None, bucketLength=None, lshModel=None, trainDataset=None, knn_model=None):\n",
    "        super(CustomLSH_Model, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, knn_model=None, inputCol=None, predictionCol=None,bucketLength=None,\n",
    "                numHashTables=None, lshModel=None, trainDataset=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)  \n",
    "        \n",
    "    \n",
    "    def add_knn_true(self,spark_dataset, knn_model, train_data):\n",
    "        df = spark_dataset.toPandas()\n",
    "        train_ds = train_data.toPandas()\n",
    "\n",
    "        features = df.features\n",
    "               \n",
    "        def get_true(feature_id):\n",
    "            #knn_indexes array of array of neighbours of each feature_id\n",
    "            knn_indexes = (knn_model.kneighbors(features[feature_id].toArray().reshape(1, -1), n_neighbors = 5, return_distance = False)).tolist()[0]\n",
    "            #form a list of ids\n",
    "            arr_of_indexes = []\n",
    "            for ind in knn_indexes:\n",
    "                arr_of_indexes.append(int(train_ds.id[ind]))\n",
    "                \n",
    "            return arr_of_indexes\n",
    "        \n",
    "        #fill pandas \n",
    "        res = [get_true(i) for i in range(0, len(df))]\n",
    "        df['knn_true'] = res\n",
    "        out_df = spark.createDataFrame(df)\n",
    "        return out_df\n",
    "        \n",
    "\n",
    "    def get_lsh_neighbors(self, dataset):\n",
    "        df = dataset.toPandas()\n",
    "            \n",
    "        def get_true_lsh(feature):\n",
    "            results = self.lshModel.approxNearestNeighbors(dataset = self.trainDataset, key = feature, numNearestNeighbors = 5, distCol = 'distance')\n",
    "            res_list = results.select('id').collect()\n",
    "            res_array = []\n",
    "            for res in res_list:\n",
    "                res_array.append(int(res[0]))\n",
    "            return res_array\n",
    "        \n",
    "        arr = [get_true_lsh(row[self.inputCol]) for index, row in df.iterrows()]\n",
    "        \n",
    "        df['lsh_true'] = arr\n",
    "        \n",
    "        out_df = spark.createDataFrame(df)\n",
    "        return out_df\n",
    "    \n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        self.inputCol = self.getInputCol()\n",
    "        outputCol = self.getPredictionCol()\n",
    "        self.lshModel = self.getLshModel()\n",
    "        self.trainDataset = self.getTrainDataset()\n",
    "        # adds knn_true to dataframe\n",
    "        tested_knn = self.add_knn_true(dataset, self.getKnnModel(), self.trainDataset)\n",
    "#         srote inc class variable\n",
    "        self.tested_knn = tested_knn\n",
    "\n",
    "        tested_lsh = self.get_lsh_neighbors(tested_knn)\n",
    "        \n",
    "        return tested_lsh\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSH(Estimator, HasInputCol, \n",
    "        HasPredictionCol, HasNumHashTables, HasLshModel,\n",
    "        DefaultParamsReadable, DefaultParamsWritable, HasBucketLength, HasKnnModel):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, predictionCol=None, numHashTables=10, bucketLength=10):\n",
    "        super(CustomLSH, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    def setInputCol(self, value):\n",
    "        return self._set(inputCol=value)\n",
    "\n",
    "    # Required in Spark >= 3.0\n",
    "    def setPredictionCol(self, value):\n",
    "        return self._set(predictionCol=value)\n",
    "    \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, predictionCol=None, numHashTables=10, bucketLength=10):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)  \n",
    "    \n",
    "    def get_knn_column(self, spark_dataset):\n",
    "#         print(spark_dataset.show(3))\n",
    "        \"\"\"\n",
    "        trains on train part of dataset.features and returns 5 neighbours for each entry\n",
    "        \"\"\"\n",
    "        df = spark_dataset.toPandas()\n",
    "        features = df.features.to_list()\n",
    "\n",
    "        nbrs = NearestNeighbors(n_neighbors=6)\n",
    "        nbrs = nbrs.fit(features)\n",
    "\n",
    "        return nbrs\n",
    "    \n",
    "    def _fit(self, dataset):\n",
    "        # get knn model as ground truth\n",
    "        knn_model = self.get_knn_column(dataset)\n",
    "        \n",
    "        inputCol = self.getInputCol()\n",
    "        lsh = BucketedRandomProjectionLSH(inputCol = inputCol, outputCol=\"hashes\", seed = 1, bucketLength = self.getBucketLength(), numHashTables = self.getNumHashTables())\n",
    "        LSH_res = lsh.fit(dataset)\n",
    "        LSH_Transform = LSH_res.transform(dataset)\n",
    "\n",
    "        return CustomLSH_Model(\n",
    "            inputCol = inputCol,\n",
    "            bucketLength = self.getBucketLength(), \n",
    "            numHashTables = self.getNumHashTables(),\n",
    "            lshModel = LSH_res,\n",
    "            trainDataset = LSH_Transform,\n",
    "            predictionCol = self.getPredictionCol(),\n",
    "            knn_model = knn_model)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEvaluator(Evaluator):\n",
    "\n",
    "    def __init__(self, predictionCol=\"prediction\", labelCol=\"label\"):\n",
    "        self.predictionCol = predictionCol\n",
    "        self.labelCol = labelCol\n",
    "\n",
    "    def _evaluate(self, dataset):\n",
    "        \n",
    "#         print(sum(F.when(dataset[self.predictionCol]==dataset[self.labelCol],1).otherwise(0)))\n",
    "\n",
    "\n",
    "        res_array_intersect = dataset.select(F.array_intersect(dataset[self.predictionCol], dataset[self.labelCol])).collect()\n",
    "  #         array_intersect(lsh_true, knn_true)\n",
    "        res_intersect_dataframe = spark.createDataFrame(res_array_intersect)\n",
    "        \n",
    "        res_intersect_dataframe = res_intersect_dataframe.withColumn('lsh_performance', F.size(F.col('array_intersect(lsh_true, knn_true)'))).drop('array_intersect(lsh_true, knn_true)')\n",
    "        \n",
    "#         res_array_intersect = F.array_intersect(dataset[self.predictionCol], dataset[self.labelCol]).collect()\n",
    "#         print(res_array_intersect.show(1))\n",
    "    \n",
    "#         dataset.withColumn(\"lsh_performance\", F.array_intersect(dataset[self.predictionCol], dataset[self.labelCol]).collect() )\n",
    "#         dataset = dataset.withColumn(\"lsh_performance\", sum(F.when(dataset[self.predictionCol]==dataset[self.labelCol],1).otherwise(0)))\n",
    "\n",
    "\n",
    "        res = res_intersect_dataframe.select(\"lsh_performance\").groupBy().sum().collect()[0]['sum(lsh_performance)']/(dataset.count() * 5)\n",
    "        print(\"Accuracy: \" + \"{:.7%}\".format(res))\n",
    "        return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.2330435%\n",
      "Accuracy: 94.1704348%\n",
      "Accuracy: 94.2608696%\n",
      "Accuracy: 96.5356522%\n",
      "Accuracy: 96.5634783%\n",
      "Accuracy: 96.5147826%\n",
      "Accuracy: 96.7721739%\n",
      "Accuracy: 96.8139130%\n",
      "Accuracy: 96.8000000%\n",
      "Time elapsed 2431.55 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# test\n",
    "\n",
    "# listings_test = listings_clear.limit(1000)\n",
    "# print(listings_test.show(1))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='name', outputCol='words')\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='tokenized')\n",
    "word2vec = Word2Vec(vectorSize = 20, inputCol=remover.getOutputCol(), outputCol='features')\n",
    "lsh = CustomLSH().setInputCol(word2vec.getOutputCol())\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer,remover, word2vec, lsh])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lsh.numHashTables, [1, 2, 3]) \\\n",
    "    .addGrid(lsh.bucketLength, [5, 6, 7]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "# crossval = CrossValidator(estimator=pipeline,\n",
    "#                           estimatorParamMaps=paramGrid,\n",
    "#                           evaluator=BinaryClassificationEvaluator(),\n",
    "#                           numFolds=2)\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=CustomEvaluator(predictionCol='lsh_true', labelCol='knn_true'),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "start = time.time()\n",
    "cvModel = tvs.fit(listings_clear)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time elapsed %.2f seconds\" % (end - start) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy: 94.2330435%\n",
    "# Accuracy: 94.1704348%\n",
    "# Accuracy: 94.2608696%\n",
    "# Accuracy: 96.5356522%\n",
    "# Accuracy: 96.5634783%\n",
    "# Accuracy: 96.5147826%\n",
    "# Accuracy: 96.7721739%\n",
    "# Accuracy: 96.8139130%\n",
    "# Accuracy: 96.8000000%\n",
    "# Time elapsed 2431.55 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 94.2330435% \n",
    "\n",
    "Accuracy: 94.1704348% \n",
    "\n",
    "Accuracy: 94.2608696% \n",
    "\n",
    "Accuracy: 96.5356522% \n",
    "\n",
    "Accuracy: 96.5634783% \n",
    "\n",
    "Accuracy: 96.5147826% \n",
    "\n",
    "Accuracy: 96.7721739% \n",
    "\n",
    "Accuracy: 96.8139130% \n",
    "\n",
    "Accuracy: 96.8000000% \n",
    "\n",
    "Time elapsed 2431.55 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is test of F.array_intersect\n",
    "# from pyspark.sql import Row\n",
    "# c1_name = 'c1'\n",
    "# c2_name = 'c2'\n",
    "\n",
    "# df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"]), Row(c1=[\"b\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
    "# list_inter = df.select(F.array_intersect(df[c1_name], df[c2_name])).collect()\n",
    "# print(list_inter)\n",
    "# len(list_inter[1]['array_intersect(c1, c2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+---------+-------------------+--------------+--------+---------+------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+\n",
      "| id|                name|host_id|host_name|neighbourhood_group| neighbourhood|latitude|longitude|   room_type|price|minimum_nights|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|\n",
      "+---+--------------------+-------+---------+-------------------+--------------+--------+---------+------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+\n",
      "|  1|Comf. double room...|  73163|   Andres|       Ciutat Vella|el Barri Gòtic|41.37973|  2.17631|Private room|   55|             3|                2| 2017-11-06|             0.05|                             3|             139|\n",
      "+---+--------------------+-------+---------+-------------------+--------------+--------+---------+------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings_clear.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomLSH_Model' object has no attribute 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a315a20e8230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistings_clear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training time %.2f seconds\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2.0.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9dfe30fa631b>\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0moutputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPredictionCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlshModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLshModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36mgetInputCol\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mGets\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0minputCol\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mits\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36mgetOrDefault\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mdefault\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mRaises\u001b[0m \u001b[0man\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneither\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolveParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36m_resolveParam\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot resolve %r as a param.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36mgetParam\u001b[0;34m(self, paramName)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mGets\u001b[0m \u001b[0ma\u001b[0m \u001b[0mparam\u001b[0m \u001b[0mby\u001b[0m \u001b[0mits\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomLSH_Model' object has no attribute 'features'"
     ]
    }
   ],
   "source": [
    "best_model = cvModel.bestModel\n",
    "\n",
    "start = time.time()\n",
    "res = cvModel.transform(listings_clear)\n",
    "end = time.time()\n",
    "print(\"training time %.2f seconds\" % (end - start) )\n",
    "\n",
    "\n",
    "res_array_intersect = res.select(F.array_intersect(res['lsh_true'], res['knn_true'])).collect()\n",
    "\n",
    "res_intersect_dataframe = spark.createDataFrame(res_array_intersect)\n",
    "\n",
    "res_intersect_dataframe = res_intersect_dataframe.withColumn('lsh_performance', F.size(F.col('array_intersect(lsh_true, knn_true)'))).drop('array_intersect(lsh_true, knn_true)')\n",
    "\n",
    "res_final = res_intersect_dataframe.select(\"lsh_performance\").groupBy().sum().collect()[0]['sum(lsh_performance)']/(res_intersect_dataframe.count() * 5)\n",
    "print(\"best_model Accuracy: \" + \"{:.7%}\".format(res_final))\n",
    "\n",
    "\n",
    "bestLSHModel = best_model.stages[3]\n",
    "\n",
    "print(\"\\nBest  parameters:\")\n",
    "print(\"LSH_numHashTables %.i\" % bestLSHModel.getNumHashTables())\n",
    "print(\"LSH_bucketLength %.i\" % bestLSHModel.getBucketLength())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = new CustomEvaluator()\n",
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "query_str = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/top/uk.wikisource/all-access/2019/04/all-days'\n",
    "sqlContext = SQLContext(spark)\n",
    "req = requests.get(query_str)\n",
    "df_words = sqlContext.createDataFrame(req.json()['items'][0]['articles'])\n",
    "\n",
    "df_words = df_words.withColumn(\"id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process text\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def remove_punctuation(column):\n",
    "     return lower(regexp_replace(concat_ws(\"SEPARATORSTRING\", column),\"[_/().,$%&!?:]\", ' ')).alias('article')\n",
    "\n",
    "def do_numbers(column):\n",
    "     return lower(regexp_replace(concat_ws(\"SEPARATORSTRING\", column),\"[\\d+]\", '[number]')).alias('article')\n",
    "\n",
    "def join_words_tokens(column):\n",
    "     return lower(regexp_replace(concat_ws(\"SEPARATORSTRING\", column),\"[-'’«»]\", '')).alias('article')\n",
    "\n",
    "def remove_extraspace(column):\n",
    "     return lower(regexp_replace(concat_ws(\"SEPARATORSTRING\", column),\"\\s\\s+\", ' ')).alias('article')\n",
    "\n",
    "df_clean = df_words.select(remove_extraspace(join_words_tokens(do_numbers(remove_punctuation(col('article'))))),\n",
    "                     col('rank'),\n",
    "                     col('views'), col('id'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ukrainian stopwords\n",
    "\n",
    "stopwords_ua = pd.read_csv(\"https://github.com/skupriienko/Ukrainian-Stopwords/raw/master/stopwords_ua.txt\", header=None, names=['stopwords'])\n",
    "stopword_ua_final = list(stopwords_ua.stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+---+\n",
      "|             article|rank|views| id|\n",
      "+--------------------+----+-----+---+\n",
      "|    головна сторінка|   1|21278|  0|\n",
      "|               вірую|   2|14244|  1|\n",
      "|мойсей іван франк...|   3| 2603|  2|\n",
      "+--------------------+----+-----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.8571429%\n",
      "Accuracy: 52.5510204%\n",
      "Accuracy: 52.4489796%\n",
      "Accuracy: 52.8571429%\n",
      "Accuracy: 52.1428571%\n",
      "Accuracy: 53.1632653%\n",
      "Accuracy: 52.8571429%\n",
      "Accuracy: 51.7346939%\n",
      "Accuracy: 51.8367347%\n",
      "Time elapsed 135.60 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol = 'article', outputCol = 'words')\n",
    "remover = StopWordsRemover(inputCol = tokenizer.getOutputCol(), outputCol = 'tokenized', stopWords = stopword_ua_final)\n",
    "word2vec = Word2Vec(vectorSize = 20, inputCol = remover.getOutputCol(), outputCol = 'features')\n",
    "lsh = CustomLSH().setInputCol(word2vec.getOutputCol())\n",
    "pipeline = Pipeline(stages=[tokenizer,remover, word2vec, lsh])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lsh.numHashTables, [3, 4, 5]) \\\n",
    "    .addGrid(lsh.bucketLength, [5, 6, 7]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "# crossval = CrossValidator(estimator=pipeline,\n",
    "#                           estimatorParamMaps=paramGrid,\n",
    "#                           evaluator=BinaryClassificationEvaluator(),\n",
    "#                           numFolds=2)\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=CustomEvaluator(predictionCol='lsh_true', labelCol='knn_true'),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "start = time.time()\n",
    "cvModel2 = tvs.fit(df_clean)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time elapsed %.2f seconds\" % (end - start) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy: 52.8571429%\n",
    "# Accuracy: 52.5510204%\n",
    "# Accuracy: 52.4489796%\n",
    "# Accuracy: 52.8571429%\n",
    "# Accuracy: 52.1428571%\n",
    "# Accuracy: 53.1632653%\n",
    "# Accuracy: 52.8571429%\n",
    "# Accuracy: 51.7346939%\n",
    "# Accuracy: 51.8367347%\n",
    "# Time elapsed 135.60 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best  parameters:\n",
      "LSH_numHashTables 4\n",
      "LSH_bucketLength 7\n"
     ]
    }
   ],
   "source": [
    "best_model = cvModel2.bestModel\n",
    "\n",
    "# start = time.time()\n",
    "# res = cvModel.transform(listings_clear)\n",
    "# end = time.time()\n",
    "# print(\"training time %.2f seconds\" % (end - start) )\n",
    "\n",
    "\n",
    "# res_array_intersect = res.select(F.array_intersect(res[self.predictionCol], res[self.labelCol])).collect()\n",
    "\n",
    "# res_intersect_dataframe = spark.createDataFrame(res_array_intersect)\n",
    "\n",
    "# res_intersect_dataframe = res_intersect_dataframe.withColumn('lsh_performance', F.size(F.col('array_intersect(lsh_true, knn_true)'))).drop('array_intersect(lsh_true, knn_true)')\n",
    "\n",
    "# res_final = res_intersect_dataframe.select(\"lsh_performance\").groupBy().sum().collect()[0]['sum(lsh_performance)']/(res_intersect_dataframe.count() * 5)\n",
    "# print(\"best_model Accuracy: \" + \"{:.7%}\".format(res_final))\n",
    "\n",
    "\n",
    "bestLSHModel = best_model.stages[3]\n",
    "\n",
    "print(\"\\nBest  parameters:\")\n",
    "print(\"LSH_numHashTables %.i\" % bestLSHModel.getNumHashTables())\n",
    "print(\"LSH_bucketLength %.i\" % bestLSHModel.getBucketLength())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.5510204%\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"Nearest Neighbors\" among (article, rank, views, id, words, tokenized, features, knn_true, lsh_true);",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-af4cf2052277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mwiki_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvs_test_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mwiki_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwiki_test_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"equal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_test_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Nearest Neighbors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mwiki_test_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ground_true'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwiki_test_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"equal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sum(equal)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mwiki_test_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy of model with Airbnb parameters on Wiki dataset is: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"{:.2%}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \"\"\"\n\u001b[1;32m   1379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"Nearest Neighbors\" among (article, rank, views, id, words, tokenized, features, knn_true, lsh_true);"
     ]
    }
   ],
   "source": [
    "#AIRBNB best params\n",
    "\n",
    "tokenizer = Tokenizer(inputCol = 'article', outputCol = 'words')\n",
    "remover = StopWordsRemover(inputCol = tokenizer.getOutputCol(), outputCol = 'tokenized', stopWords = stopword_ua_final)\n",
    "word2vec = Word2Vec(vectorSize = 20, inputCol = remover.getOutputCol(), outputCol = 'features')\n",
    "lsh = CustomLSH(inputCol=word2vec.getOutputCol())\n",
    "# lsh = CustomLSH().setInputCol(\"features\")\n",
    "pipeline = Pipeline(stages=[tokenizer,remover, word2vec, lsh])\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(lsh.numHashTables, [5]).addGrid(lsh.bucketLength, [3]).build()\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                           evaluator=CustomEvaluator(predictionCol='lsh_true', labelCol='knn_true'),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "tvs_test_model = tvs.fit(df_clean)\n",
    "wiki_test_data = tvs_test_model.transform(df_clean)\n",
    "\n",
    "wiki_test_data = wiki_test_data.withColumn(\"equal\", F.when(wiki_test_data['Nearest Neighbors']==wiki_test_data['ground_true'],1).otherwise(0))\n",
    "acc = wiki_test_data.select(\"equal\").groupBy().sum().collect()[0]['sum(equal)']/wiki_test_data.count()\n",
    "print(\"Accuracy of model with Airbnb parameters on Wiki dataset is: \"+\"{:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
