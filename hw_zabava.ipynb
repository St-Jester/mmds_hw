{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover,Word2Vec,BucketedRandomProjectionLSH\n",
    "from pyspark.ml import Pipeline, Estimator, Model\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,Evaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable \n",
    "from pyspark.ml.param.shared import *\n",
    "from pyspark import keyword_only \n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-19 00:03:08--  http://data.insideairbnb.com/spain/catalonia/barcelona/2020-10-12/visualisations/listings.csv\n",
      "Resolving data.insideairbnb.com (data.insideairbnb.com)... 52.216.88.178\n",
      "Connecting to data.insideairbnb.com (data.insideairbnb.com)|52.216.88.178|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3070172 (2.9M) [application/csv]\n",
      "Saving to: ‘listings_barc.csv’\n",
      "\n",
      "listings_barc.csv   100%[===================>]   2.93M  1.36MB/s    in 2.2s    \n",
      "\n",
      "2020-11-19 00:03:11 (1.36 MB/s) - ‘listings_barc.csv’ saved [3070172/3070172]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"http://data.insideairbnb.com/spain/catalonia/barcelona/2020-10-12/visualisations/listings.csv\" -O listings_barc.csv\n",
    "# !wget http://data.insideairbnb.com/spain/catalonia/barcelona/2020-10-12/visualisations/reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# use pandas to read and clean data, because spark has some issues reading the csv file\n",
    "df = pd.read_csv(\"listings_barc.csv\")\n",
    "# drop all na so no errors like \n",
    "df.dropna(inplace = True)\n",
    "\n",
    "# make indexes start form 1\n",
    "df['id'] = np.arange(1, len(df) + 1)\n",
    "\n",
    "# df.to_csv(\"listings_barc.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_clear = spark.createDataFrame(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to get KNN_model (returned in CustomLSH::_fit)\n",
    "class HasKnnModel(Params):\n",
    "    knn_model = Param(Params._dummy(), \"knn_model\", \"knn_model\")\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasKnnModel, self).__init__()\n",
    "\n",
    "    def setKnnModel(self, value):\n",
    "        return self._set(knn_model=value)\n",
    "\n",
    "    def getKnnModel(self):\n",
    "        return self.getOrDefault(self.knn_model)\n",
    "\n",
    "class HasNumHashTables(Params):\n",
    "    numHashTables = Param(Params._dummy(), \"numHashTables\", \"numHashTables\", \n",
    "        typeConverter=TypeConverters.toInt)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasNumHashTables, self).__init__()\n",
    "\n",
    "    def setNumHashTables(self, value):\n",
    "        return self._set(numHashTables=value)\n",
    "\n",
    "    def getNumHashTables(self):\n",
    "        return self.getOrDefault(self.numHashTables)\n",
    "    \n",
    "class HasLshModel(Params):\n",
    "    lshModel = Param(Params._dummy(), \"lshModel\", \"lshModel\")\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasLshModel, self).__init__()\n",
    "\n",
    "    def setLshModel(self, value):\n",
    "        return self._set(lshModel=value)\n",
    "\n",
    "    def getLshModel(self):\n",
    "        return self.getOrDefault(self.lshModel)\n",
    "    \n",
    "class HasTrainDataset(Params):\n",
    "    trainDataset = Param(Params._dummy(), \"trainDataset\", \"trainDataset\")\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasTrainDataset, self).__init__()\n",
    "\n",
    "    def setTrainDataset(self, value):\n",
    "        return self._set(trainDataset=value)\n",
    "\n",
    "    def getTrainDataset(self):\n",
    "        return self.getOrDefault(self.trainDataset)\n",
    "    \n",
    "    \n",
    "class HasBucketLength(Params):\n",
    "    bucketLength = Param(Params._dummy(), \"bucketLength\", \"bucketLength\", \n",
    "        typeConverter=TypeConverters.toInt)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasBucketLength, self).__init__()\n",
    "\n",
    "    def setBucketLength(self, value):\n",
    "        return self._set(bucketLength=value)\n",
    "\n",
    "    def getBucketLength(self):\n",
    "        return self.getOrDefault(self.bucketLength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomLSH_Model(Model, HasInputCol, HasPredictionCol,\n",
    "        HasNumHashTables, HasLshModel,HasTrainDataset,\n",
    "        DefaultParamsReadable, DefaultParamsWritable, HasBucketLength, HasKnnModel):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self,inputCol=None, predictionCol=None,\n",
    "                numHashTables=None, bucketLength=None, lshModel=None, trainDataset=None, knn_model=None):\n",
    "        super(CustomLSH_Model, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, knn_model=None, inputCol=None, predictionCol=None,bucketLength=None,\n",
    "                numHashTables=None, lshModel=None, trainDataset=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)  \n",
    "        \n",
    "    \n",
    "    def add_knn_true(self,spark_dataset, knn_model, train_data):\n",
    "        df = spark_dataset.toPandas()\n",
    "        train_ds = train_data.toPandas()\n",
    "\n",
    "        features = df.features\n",
    "               \n",
    "        def get_true(feature_id):\n",
    "            #knn_indexes array of array of neighbours of each feature_id\n",
    "            knn_indexes = (knn_model.kneighbors(features[feature_id].toArray().reshape(1, -1), n_neighbors = 5, return_distance = False)).tolist()[0]\n",
    "            #form a list of ids\n",
    "            arr_of_indexes = []\n",
    "            for ind in knn_indexes:\n",
    "                arr_of_indexes.append(int(train_ds.id[ind]))\n",
    "                \n",
    "            return arr_of_indexes\n",
    "        \n",
    "        #fill pandas \n",
    "        res = [get_true(i) for i in range(0, len(df))]\n",
    "        df['knn_true'] = res\n",
    "        out_df = spark.createDataFrame(df)\n",
    "        return out_df\n",
    "        \n",
    "\n",
    "    def get_lsh_neighbors(self, dataset):\n",
    "        df = dataset.toPandas()\n",
    "            \n",
    "        def get_true_lsh(feature):\n",
    "            results = self.lshModel.approxNearestNeighbors(dataset = self.trainDataset, key = feature, numNearestNeighbors = 5, distCol = 'distance')\n",
    "            res_list = results.select('id').collect()\n",
    "            res_array = []\n",
    "            for res in res_list:\n",
    "                res_array.append(int(res[0]))\n",
    "            return res_array\n",
    "        \n",
    "        arr = [get_true_lsh(row[self.inputCol]) for index, row in df.iterrows()]\n",
    "        \n",
    "        df['lsh_true'] = arr\n",
    "        \n",
    "        out_df = spark.createDataFrame(df)\n",
    "        return out_df\n",
    "    \n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        self.inputCol = self.getInputCol()\n",
    "        outputCol = self.getPredictionCol()\n",
    "        self.lshModel = self.getLshModel()\n",
    "        self.trainDataset = self.getTrainDataset()\n",
    "        # adds knn_true to dataframe\n",
    "        tested_knn = self.add_knn_true(dataset, self.getKnnModel(), self.trainDataset)\n",
    "#         store in class variable\n",
    "        self.tested_knn = tested_knn\n",
    "#         print(self.trainDataset.count())\n",
    "#         print(tested_knn.count())\n",
    "#         print(dataset.count())\n",
    "        \n",
    "        tested_lsh = self.get_lsh_neighbors(tested_knn)\n",
    "        \n",
    "#         print(tested_lsh.count())\n",
    "        return tested_lsh\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSH(Estimator, HasInputCol, \n",
    "        HasPredictionCol, HasNumHashTables, HasLshModel,\n",
    "        DefaultParamsReadable, DefaultParamsWritable, HasBucketLength, HasKnnModel):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, predictionCol=None, numHashTables=10, bucketLength=10):\n",
    "        super(CustomLSH, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    def setInputCol(self, value):\n",
    "        return self._set(inputCol=value)\n",
    "\n",
    "    # Required in Spark >= 3.0\n",
    "    def setPredictionCol(self, value):\n",
    "        return self._set(predictionCol=value)\n",
    "    \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, predictionCol=None, numHashTables=10, bucketLength=10):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)  \n",
    "    \n",
    "    def get_knn_column(self, spark_dataset):\n",
    "#         print(spark_dataset.show(3))\n",
    "        \"\"\"\n",
    "        trains on train part of dataset.features and returns 5 neighbours for each entry\n",
    "        \"\"\"\n",
    "        df = spark_dataset.toPandas()\n",
    "        features = df.features.to_list()\n",
    "\n",
    "        nbrs = NearestNeighbors(n_neighbors=6)\n",
    "        nbrs = nbrs.fit(features)\n",
    "\n",
    "        return nbrs\n",
    "    \n",
    "    def _fit(self, dataset):\n",
    "        # get knn model as ground truth\n",
    "        knn_model = self.get_knn_column(dataset)\n",
    "#         print(f\"_fit {dataset.count()}\")\n",
    "        inputCol = self.getInputCol()\n",
    "        lsh = BucketedRandomProjectionLSH(inputCol = inputCol, outputCol=\"hashes\", seed = 1, bucketLength = self.getBucketLength(), numHashTables = self.getNumHashTables())\n",
    "        LSH_res = lsh.fit(dataset)\n",
    "        LSH_Transform = LSH_res.transform(dataset)\n",
    "\n",
    "        return CustomLSH_Model(\n",
    "            inputCol = inputCol,\n",
    "            bucketLength = self.getBucketLength(), \n",
    "            numHashTables = self.getNumHashTables(),\n",
    "            lshModel = LSH_res,\n",
    "            trainDataset = LSH_Transform,\n",
    "            predictionCol = self.getPredictionCol(),\n",
    "            knn_model = knn_model)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEvaluator(Evaluator):\n",
    "\n",
    "    def __init__(self, predictionCol=\"prediction\", labelCol=\"label\"):\n",
    "        self.predictionCol = predictionCol\n",
    "        self.labelCol = labelCol\n",
    "\n",
    "    def _evaluate(self, dataset):\n",
    "        \n",
    "#         print(sum(F.when(dataset[self.predictionCol]==dataset[self.labelCol],1).otherwise(0)))\n",
    "        res_array_intersect = dataset.select(F.array_intersect(dataset[self.predictionCol], dataset[self.labelCol])).collect()\n",
    "  #         array_intersect(lsh_true, knn_true)\n",
    "        res_intersect_dataframe = spark.createDataFrame(res_array_intersect)\n",
    "        \n",
    "        res_intersect_dataframe = res_intersect_dataframe.withColumn('lsh_performance', F.size(F.col('array_intersect(lsh_true, knn_true)'))).drop('array_intersect(lsh_true, knn_true)')\n",
    "        \n",
    "#         res_array_intersect = F.array_intersect(dataset[self.predictionCol], dataset[self.labelCol]).collect()\n",
    "#         print(res_array_intersect.show(1))\n",
    "    \n",
    "#         dataset.withColumn(\"lsh_performance\", F.array_intersect(dataset[self.predictionCol], dataset[self.labelCol]).collect() )\n",
    "#         dataset = dataset.withColumn(\"lsh_performance\", sum(F.when(dataset[self.predictionCol]==dataset[self.labelCol],1).otherwise(0)))\n",
    "        res = res_intersect_dataframe.select(\"lsh_performance\").groupBy().sum().collect()[0]['sum(lsh_performance)']/(dataset.count() * 5)\n",
    "        print(\"Accuracy: \" + \"{:.7%}\".format(res))\n",
    "        return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.6302432%\n",
      "Accuracy: 96.7289390%\n",
      "Accuracy: 96.6231935%\n",
      "Accuracy: 96.6513923%\n",
      "Accuracy: 96.6443426%\n",
      "Accuracy: 96.7289390%\n",
      "Accuracy: 96.6654917%\n",
      "Accuracy: 96.6795911%\n",
      "Accuracy: 96.6513923%\n",
      "Time elapsed 2746.79 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='name', outputCol='words')\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='tokenized')\n",
    "word2vec = Word2Vec(vectorSize = 20, inputCol=remover.getOutputCol(), outputCol='features')\n",
    "lsh = CustomLSH().setInputCol(word2vec.getOutputCol())\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, word2vec, lsh])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lsh.numHashTables, [3, 4, 5]) \\\n",
    "    .addGrid(lsh.bucketLength, [5, 7, 9]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "# crossval = CrossValidator(estimator=pipeline,\n",
    "#                           estimatorParamMaps=paramGrid,\n",
    "#                           evaluator=BinaryClassificationEvaluator(),\n",
    "#                           numFolds=2)\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=CustomEvaluator(predictionCol='lsh_true', labelCol='knn_true'),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "start = time.time()\n",
    "cvModel = tvs.fit(listings_clear)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time elapsed %.2f seconds\" % (end - start) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS shown by previous cell \n",
    "# Accuracy: 96.6302432%\n",
    "# Accuracy: 96.7289390%\n",
    "# Accuracy: 96.6231935%\n",
    "# Accuracy: 96.6513923%\n",
    "# Accuracy: 96.6443426%\n",
    "# Accuracy: 96.7289390%\n",
    "# Accuracy: 96.6654917%\n",
    "# Accuracy: 96.6795911%\n",
    "# Accuracy: 96.6513923%\n",
    "# Time elapsed 2746.79 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time 2115.45 seconds\n",
      "best_model Accuracy: 96.5921907%\n",
      "\n",
      "Best  parameters:\n",
      "LSH_numHashTables 3\n",
      "LSH_bucketLength 7\n"
     ]
    }
   ],
   "source": [
    "best_model = cvModel.bestModel\n",
    "\n",
    "start = time.time()\n",
    "res = cvModel.bestModel.transform(listings_clear)\n",
    "end = time.time()\n",
    "print(\"training time %.2f seconds\" % (end - start) )\n",
    "\n",
    "\n",
    "res_array_intersect = res.select(F.array_intersect(res['lsh_true'], res['knn_true'])).collect()\n",
    "\n",
    "res_intersect_dataframe = spark.createDataFrame(res_array_intersect)\n",
    "\n",
    "res_intersect_dataframe = res_intersect_dataframe.withColumn('lsh_performance', F.size(F.col('array_intersect(lsh_true, knn_true)'))).drop('array_intersect(lsh_true, knn_true)')\n",
    "\n",
    "res_final = res_intersect_dataframe.select(\"lsh_performance\").groupBy().sum().collect()[0]['sum(lsh_performance)']/(res_intersect_dataframe.count() * 5)\n",
    "print(\"best_model Accuracy: \" + \"{:.7%}\".format(res_final))\n",
    "\n",
    "bestLSHModel = best_model.stages[3]\n",
    "\n",
    "print(\"\\nBest  parameters:\")\n",
    "print(\"LSH_numHashTables %.i\" % bestLSHModel.getNumHashTables())\n",
    "print(\"LSH_bucketLength %.i\" % bestLSHModel.getBucketLength())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS shown by previous cell \n",
    "# best_model Accuracy: 96.5921907%\n",
    "\n",
    "# Best  parameters:\n",
    "# LSH_numHashTables 3\n",
    "# LSH_bucketLength 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is test of F.array_intersect\n",
    "# from pyspark.sql import Row\n",
    "# c1_name = 'c1'\n",
    "# c2_name = 'c2'\n",
    "\n",
    "# df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"]), Row(c1=[\"b\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
    "# list_inter = df.select(F.array_intersect(df[c1_name], df[c2_name])).collect()\n",
    "# print(list_inter)\n",
    "# len(list_inter[1]['array_intersect(c1, c2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "query_str = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/top/uk.wikisource/all-access/2019/04/all-days'\n",
    "sqlContext = SQLContext(spark)\n",
    "req = requests.get(query_str)\n",
    "df_words = sqlContext.createDataFrame(req.json()['items'][0]['articles'])\n",
    "\n",
    "df_words = df_words.withColumn(\"id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process text\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def remove_punctuation(column):\n",
    "     return lower(regexp_replace(concat_ws(\"SEPARATORSTRING\", column),\"[_/().,$%&!?:]\", ' ')).alias('article')\n",
    "\n",
    "def do_numbers(column):\n",
    "     return lower(regexp_replace(concat_ws(\"SEPARATORSTRING\", column),\"[\\d+]\", '[number]')).alias('article')\n",
    "\n",
    "def join_words_tokens(column):\n",
    "     return lower(regexp_replace(concat_ws(\"SEPARATORSTRING\", column),\"[-'’«»]\", '')).alias('article')\n",
    "\n",
    "def remove_extraspace(column):\n",
    "     return lower(regexp_replace(concat_ws(\"SEPARATORSTRING\", column),\"\\s\\s+\", ' ')).alias('article')\n",
    "\n",
    "df_clean = df_words.select(remove_extraspace(join_words_tokens(do_numbers(remove_punctuation(col('article'))))),\n",
    "                     col('rank'),\n",
    "                     col('views'), col('id'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ukrainian stopwords\n",
    "\n",
    "stopwords_ua = pd.read_csv(\"https://github.com/skupriienko/Ukrainian-Stopwords/raw/master/stopwords_ua.txt\", header=None, names=['stopwords'])\n",
    "stopword_ua_final = list(stopwords_ua.stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+---+\n",
      "|             article|rank|views| id|\n",
      "+--------------------+----+-----+---+\n",
      "|    головна сторінка|   1|21278|  0|\n",
      "|               вірую|   2|14244|  1|\n",
      "|мойсей іван франк...|   3| 2603|  2|\n",
      "+--------------------+----+-----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.7956989%\n",
      "Accuracy: 53.0107527%\n",
      "Accuracy: 52.6881720%\n",
      "Accuracy: 53.8709677%\n",
      "Accuracy: 54.0860215%\n",
      "Accuracy: 53.7634409%\n",
      "Accuracy: 54.8387097%\n",
      "Accuracy: 54.0860215%\n",
      "Accuracy: 54.3010753%\n",
      "Accuracy: 53.8709677%\n",
      "Accuracy: 53.9784946%\n",
      "Accuracy: 54.9462366%\n",
      "Accuracy: 54.0860215%\n",
      "Accuracy: 54.1935484%\n",
      "Accuracy: 53.8709677%\n",
      "Accuracy: 54.8387097%\n",
      "Time elapsed 233.77 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol = 'article', outputCol = 'words')\n",
    "remover = StopWordsRemover(inputCol = tokenizer.getOutputCol(), outputCol = 'tokenized', stopWords = stopword_ua_final)\n",
    "word2vec = Word2Vec(vectorSize = 20, inputCol = remover.getOutputCol(), outputCol = 'features')\n",
    "lsh = CustomLSH().setInputCol(word2vec.getOutputCol())\n",
    "pipeline = Pipeline(stages=[tokenizer,remover, word2vec, lsh])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lsh.numHashTables, [3, 4, 5, 6]) \\\n",
    "    .addGrid(lsh.bucketLength, [5, 6, 7, 8]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "# crossval = CrossValidator(estimator=pipeline,\n",
    "#                           estimatorParamMaps=paramGrid,\n",
    "#                           evaluator=BinaryClassificationEvaluator(),\n",
    "#                           numFolds=2)\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=CustomEvaluator(predictionCol='lsh_true', labelCol='knn_true'),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "start = time.time()\n",
    "cvModel2 = tvs.fit(df_clean)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time elapsed %.2f seconds\" % (end - start) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS shown by previous cell \n",
    "# Accuracy: 52.7956989%\n",
    "# Accuracy: 53.0107527%\n",
    "# Accuracy: 52.6881720%\n",
    "# Accuracy: 53.8709677%\n",
    "# Accuracy: 54.0860215%\n",
    "# Accuracy: 53.7634409%\n",
    "# Accuracy: 54.8387097%\n",
    "# Accuracy: 54.0860215%\n",
    "# Accuracy: 54.3010753%\n",
    "# Accuracy: 53.8709677%\n",
    "# Accuracy: 53.9784946%\n",
    "# Accuracy: 54.9462366%\n",
    "# Accuracy: 54.0860215%\n",
    "# Accuracy: 54.1935484%\n",
    "# Accuracy: 53.8709677%\n",
    "# Accuracy: 54.8387097%\n",
    "# Time elapsed 233.77 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time 132.16 seconds\n",
      "best_model Accuracy: 58.6666667%\n",
      "\n",
      "Best  parameters:\n",
      "LSH_numHashTables 5\n",
      "LSH_bucketLength 8\n"
     ]
    }
   ],
   "source": [
    "best_model = cvModel2.bestModel\n",
    "\n",
    "start = time.time()\n",
    "res = cvModel2.transform(df_clean)\n",
    "end = time.time()\n",
    "print(\"training time %.2f seconds\" % (end - start) )\n",
    "\n",
    "\n",
    "res_array_intersect = res.select(F.array_intersect(res['lsh_true'], res['knn_true'])).collect()\n",
    "\n",
    "res_intersect_dataframe = spark.createDataFrame(res_array_intersect)\n",
    "\n",
    "res_intersect_dataframe = res_intersect_dataframe.withColumn('lsh_performance', F.size(F.col('array_intersect(lsh_true, knn_true)'))).drop('array_intersect(lsh_true, knn_true)')\n",
    "\n",
    "res_final = res_intersect_dataframe.select(\"lsh_performance\").groupBy().sum().collect()[0]['sum(lsh_performance)']/(res_intersect_dataframe.count() * 5)\n",
    "print(\"best_model Accuracy: \" + \"{:.7%}\".format(res_final))\n",
    "\n",
    "bestLSHModel = best_model.stages[3]\n",
    "\n",
    "print(\"\\nBest  parameters:\")\n",
    "print(\"LSH_numHashTables %.i\" % bestLSHModel.getNumHashTables())\n",
    "print(\"LSH_bucketLength %.i\" % bestLSHModel.getBucketLength())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS shown by previous cell \n",
    "# training time 132.16 seconds\n",
    "# best_model Accuracy: 58.6666667%\n",
    "\n",
    "# Best  parameters:\n",
    "# LSH_numHashTables 5\n",
    "# LSH_bucketLength 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.4408602%\n",
      "best_model Accuracy: 59.0358974%\n"
     ]
    }
   ],
   "source": [
    "#AIRBNB best params on wiki dataset\n",
    "\n",
    "tokenizer = Tokenizer(inputCol = 'article', outputCol = 'words')\n",
    "remover = StopWordsRemover(inputCol = tokenizer.getOutputCol(), outputCol = 'tokenized', stopWords = stopword_ua_final)\n",
    "word2vec = Word2Vec(vectorSize = 20, inputCol = remover.getOutputCol(), outputCol = 'features')\n",
    "lsh = CustomLSH(inputCol=word2vec.getOutputCol())\n",
    "# lsh = CustomLSH().setInputCol(\"features\")\n",
    "pipeline = Pipeline(stages=[tokenizer,remover, word2vec, lsh])\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(lsh.numHashTables, [3]).addGrid(lsh.bucketLength, [7]).build()\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=CustomEvaluator(predictionCol='lsh_true', labelCol='knn_true'),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "test_model = tvs.fit(df_clean)\n",
    "test_model_res = test_model.transform(df_clean)\n",
    "\n",
    "res_array_intersect = test_model_res.select(F.array_intersect(test_model_res['lsh_true'], test_model_res['knn_true'])).collect()\n",
    "\n",
    "res_intersect_dataframe = spark.createDataFrame(res_array_intersect)\n",
    "\n",
    "res_intersect_dataframe = res_intersect_dataframe.withColumn('lsh_performance', F.size(F.col('array_intersect(lsh_true, knn_true)'))).drop('array_intersect(lsh_true, knn_true)')\n",
    "\n",
    "res_final = res_intersect_dataframe.select(\"lsh_performance\").groupBy().sum().collect()[0]['sum(lsh_performance)']/(res_intersect_dataframe.count() * 5)\n",
    "print(\"best_model Accuracy: \" + \"{:.7%}\".format(res_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.0000000%\n",
      "Accuracy: 80.0000000%\n",
      "Accuracy: 80.0000000%\n",
      "Accuracy: 80.9090909%\n",
      "Accuracy: 80.9090909%\n",
      "Accuracy: 80.9090909%\n",
      "Accuracy: 85.4545455%\n",
      "Accuracy: 85.4545455%\n",
      "Accuracy: 85.4545455%\n",
      "Time elapsed 20.94 seconds\n",
      "training time 14.94 seconds\n",
      "best_model Accuracy: 92.8000000%\n"
     ]
    }
   ],
   "source": [
    "# # this is test run to check that everything is working\n",
    "\n",
    "# import time\n",
    "# listings_test = listings_clear.limit(100)\n",
    "\n",
    "# tokenizer = Tokenizer(inputCol='name', outputCol='words')\n",
    "# remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='tokenized')\n",
    "# word2vec = Word2Vec(vectorSize = 20, inputCol=remover.getOutputCol(), outputCol='features')\n",
    "# lsh = CustomLSH().setInputCol(word2vec.getOutputCol())\n",
    "\n",
    "# pipeline = Pipeline(stages=[tokenizer,remover, word2vec, lsh])\n",
    "\n",
    "# paramGrid = ParamGridBuilder() \\\n",
    "#     .addGrid(lsh.numHashTables, [1, 2, 3]) \\\n",
    "#     .addGrid(lsh.bucketLength, [5, 6, 7]) \\\n",
    "#     .build()\n",
    "\n",
    "\n",
    "# # crossval = CrossValidator(estimator=pipeline,\n",
    "# #                           estimatorParamMaps=paramGrid,\n",
    "# #                           evaluator=BinaryClassificationEvaluator(),\n",
    "# #                           numFolds=2)\n",
    "\n",
    "# tvs_test = TrainValidationSplit(estimator = pipeline,\n",
    "#                            estimatorParamMaps = paramGrid,\n",
    "#                            evaluator = CustomEvaluator(predictionCol='lsh_true', labelCol='knn_true'),\n",
    "#                            # 80% of the data will be used for training, 20% for validation.\n",
    "#                            trainRatio = 0.8)\n",
    "\n",
    "# start = time.time()\n",
    "# cvModel_test = tvs_test.fit(listings_test)\n",
    "# end = time.time()\n",
    "\n",
    "# print(\"Time elapsed %.2f seconds\" % (end - start) )\n",
    "\n",
    "# best_model = cvModel_test.bestModel\n",
    "\n",
    "# start = time.time()\n",
    "# res = cvModel_test.transform(listings_test)\n",
    "# end = time.time()\n",
    "# print(\"training time %.2f seconds\" % (end - start) )\n",
    "\n",
    "\n",
    "# res_array_intersect = res.select(F.array_intersect(res['lsh_true'], res['knn_true'])).collect()\n",
    "\n",
    "# res_intersect_dataframe = spark.createDataFrame(res_array_intersect)\n",
    "\n",
    "# res_intersect_dataframe = res_intersect_dataframe.withColumn('lsh_performance', F.size(F.col('array_intersect(lsh_true, knn_true)'))).drop('array_intersect(lsh_true, knn_true)')\n",
    "\n",
    "# res_final = res_intersect_dataframe.select(\"lsh_performance\").groupBy().sum().collect()[0]['sum(lsh_performance)']/(res_intersect_dataframe.count() * 5)\n",
    "# print(\"best_model Accuracy: \" + \"{:.7%}\".format(res_final))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
